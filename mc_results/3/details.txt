╔════════╗
║  BINS  ║
╚════════╝
{'ball_velocity_x': 5,
 'ball_velocity_y': 5,
 'ball_x': 12,
 'ball_y': 12,
 'cpu_y': 1,
 'player_velocity': 1,
 'player_y': 12}
╔══════════╗
║  LIMITS  ║
╚══════════╝
{'ball_velocity_x': (-220, 220),
 'ball_velocity_y': (-148, 148),
 'ball_x': (0, 220),
 'ball_y': (0, 148),
 'cpu_y': (0, 148),
 'player_velocity': (-59.2, 59.2),
 'player_y': (0, 148)}
╔═════════╗
║  AGENT  ║
╚═════════╝
Monte Carlo Agent
num_states: 43200
num_actions: 3
visit_mode: every
discount_factor: 0.95
alpha_mode: inverse
alpha: 1
alpha_power: 0.6666666666666666

{'ball_velocity_x': array([-220., -110.,    0.,  110.,  220.]),
 'ball_velocity_y': array([-148.,  -74.,    0.,   74.,  148.]),
 'ball_x': array([  0.,  20.,  40.,  60.,  80., 100., 120., 140., 160., 180., 200.,
       220.]),
 'ball_y': array([  0.        ,  13.45454545,  26.90909091,  40.36363636,
        53.81818182,  67.27272727,  80.72727273,  94.18181818,
       107.63636364, 121.09090909, 134.54545455, 148.        ]),
 'cpu_y': array([0.]),
 'player_velocity': array([-59.2]),
 'player_y': array([  0.        ,  13.45454545,  26.90909091,  40.36363636,
        53.81818182,  67.27272727,  80.72727273,  94.18181818,
       107.63636364, 121.09090909, 134.54545455, 148.        ])}
episode 1:1000: avg reward = -301.0131330264711, avg length = 6944.579
episode 1001:2000: avg reward = -290.38297116487945, avg length = 7482.307
episode 2001:3000: avg reward = -282.91524887355064, avg length = 7362.893
episode 3001:4000: avg reward = -285.0477373021994, avg length = 7459.289
episode 4001:5000: avg reward = -285.6398161153412, avg length = 7496.485
episode 5001:6000: avg reward = -276.9745768961577, avg length = 7256.399
episode 6001:7000: avg reward = -279.91211279070353, avg length = 7346.657
episode 7001:8000: avg reward = -291.78922201313316, avg length = 7690.476
episode 8001:9000: avg reward = -285.6635138083199, avg length = 7495.381
episode 9001:10000: avg reward = -280.41793938026836, avg length = 7380.73
episode 10001:11000: avg reward = -281.4751989477403, avg length = 7398.07
episode 11001:12000: avg reward = -281.1897123845967, avg length = 7362.367
episode 12001:13000: avg reward = -283.6601970997507, avg length = 7438.706
episode 13001:14000: avg reward = -286.1601477844087, avg length = 7509.596
episode 14001:15000: avg reward = -287.17986922224884, avg length = 7550.581
episode 15001:16000: avg reward = -281.4836404055891, avg length = 7392.576
episode 16001:17000: avg reward = -284.86167230499905, avg length = 7487.735
episode 17001:18000: avg reward = -283.9698679575099, avg length = 7474.188
episode 18001:19000: avg reward = -283.5454902436624, avg length = 7462.315
episode 19001:20000: avg reward = -283.7653218367863, avg length = 7433.221
episode 20001:21000: avg reward = -280.60593086093075, avg length = 7365.442
episode 21001:22000: avg reward = -287.5563941001396, avg length = 7565.475
episode 22001:23000: avg reward = -283.24753497386416, avg length = 7461.371
episode 23001:24000: avg reward = -280.88232525742205, avg length = 7369.046
episode 24001:25000: avg reward = -282.2273174601269, avg length = 7407.944
episode 25001:26000: avg reward = -284.6638932761979, avg length = 7483.855
episode 26001:27000: avg reward = -283.82589107654184, avg length = 7461.647
episode 27001:28000: avg reward = -285.3217637886736, avg length = 7502.526
episode 28001:29000: avg reward = -280.5118285113048, avg length = 7361.009
episode 29001:30000: avg reward = -287.7995171652369, avg length = 7583.753
episode 30001:31000: avg reward = -282.6034206265242, avg length = 7415.097
episode 31001:32000: avg reward = -281.90927045605844, avg length = 7425.829
episode 32001:33000: avg reward = -281.08185112798196, avg length = 7396.981
^CSaving current Q-values...
7 hrs
